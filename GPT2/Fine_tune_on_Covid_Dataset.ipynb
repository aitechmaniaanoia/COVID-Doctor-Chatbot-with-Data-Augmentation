{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-tune on Covid Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yGJHOZYOWZHR",
        "4hyTyW7Le2Sb",
        "IuzSROqxjUKM",
        "X_qYqlTe9yx2",
        "1155nQJ_GShb",
        "pkvMNnrnVHQw",
        "6eDkPEuvbD47"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6d0444a6a79b49ceb5e0cb080a32d15d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_88e6c5d1b3024abda6e125261bb588db",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8298e6a17ff046b1a3c94b1757b0de03",
              "IPY_MODEL_61cf083be32745c285f19cfec6b5e4d7"
            ]
          }
        },
        "88e6c5d1b3024abda6e125261bb588db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8298e6a17ff046b1a3c94b1757b0de03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0edf85433c0c44c19764c68fddf8c772",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 641,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 641,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a959e533491b4c158a64e406851d593a"
          }
        },
        "61cf083be32745c285f19cfec6b5e4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_66253844087d459babaf61c3f4f1e2ac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 641/641 [00:00&lt;00:00, 1.64kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed79608317e74004ae009c8237e88998"
          }
        },
        "0edf85433c0c44c19764c68fddf8c772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a959e533491b4c158a64e406851d593a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66253844087d459babaf61c3f4f1e2ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed79608317e74004ae009c8237e88998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2eed819c3df34d34abd97f1ddb1f4a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2ef500b8fe5949939e762417d336d4dc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_539b0732ffd44f0a871ad88596612d6b",
              "IPY_MODEL_2f2fad4c4e674062b09792c7b4aff5d8"
            ]
          }
        },
        "2ef500b8fe5949939e762417d336d4dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "539b0732ffd44f0a871ad88596612d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4ee2b429ea7c4690bdcfaece23ce0106",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc2ec07ae6d54383a1f70ca428ab0fa6"
          }
        },
        "2f2fad4c4e674062b09792c7b4aff5d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0b5bcdd7660c49e09c17e5d3d9128ddf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 8.33MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bbc6db6f6a474cc883adcfa644686ed4"
          }
        },
        "4ee2b429ea7c4690bdcfaece23ce0106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc2ec07ae6d54383a1f70ca428ab0fa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b5bcdd7660c49e09c17e5d3d9128ddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bbc6db6f6a474cc883adcfa644686ed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94b0087ed7c34ee6839a2fac6941aae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_32b147b9d12d4209acc6921449624e76",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b59a022c45ee483cb179fd79eb49d2a2",
              "IPY_MODEL_00cbf796c9e64836b1ed6d981a95694e"
            ]
          }
        },
        "32b147b9d12d4209acc6921449624e76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b59a022c45ee483cb179fd79eb49d2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e6c73e0028c64cd79b4256ae6e27faa0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7c81f46882f2483ba7ae51aa241d27e9"
          }
        },
        "00cbf796c9e64836b1ed6d981a95694e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4c1c1ed302474e0e82b97fa82f27dba0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 2.56MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b2875ad278a84915bef0a3bc255565b5"
          }
        },
        "e6c73e0028c64cd79b4256ae6e27faa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7c81f46882f2483ba7ae51aa241d27e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c1c1ed302474e0e82b97fa82f27dba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b2875ad278a84915bef0a3bc255565b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d6fed40440140ffa38163cf29b0ac42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_84c10d0d08734f88a12604a9b6d8beeb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_948b24a8bbc4470dbae294f5da7d07ea",
              "IPY_MODEL_2ac0c95b66cb48c48e59c93f626ab0bd"
            ]
          }
        },
        "84c10d0d08734f88a12604a9b6d8beeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "948b24a8bbc4470dbae294f5da7d07ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2ab2ce6738434e2ab7cac1d617b1e7b5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 351265583,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 351265583,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e755f8252c544e3940692d8374523de"
          }
        },
        "2ac0c95b66cb48c48e59c93f626ab0bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_76bf9b49716e4a04a4875cc75516f846",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 351M/351M [00:05&lt;00:00, 61.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec596280489f4b5baf64aba50679d0ee"
          }
        },
        "2ab2ce6738434e2ab7cac1d617b1e7b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e755f8252c544e3940692d8374523de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "76bf9b49716e4a04a4875cc75516f846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec596280489f4b5baf64aba50679d0ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e76773810d714ad9b4d9954f37c53bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_97e3a1002cc748babfe474355a17d1be",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_064459aa1bde4b9388bfde309dde6a93",
              "IPY_MODEL_1b4c9cddfd4f4f89a64c40c1e429bd5f"
            ]
          }
        },
        "97e3a1002cc748babfe474355a17d1be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "064459aa1bde4b9388bfde309dde6a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4310251017ae4a6fbd275a6d69ecf1c2",
            "_dom_classes": [],
            "description": "Evaluating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 15,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 15,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40103217096f473c93e488cd15eaa2a8"
          }
        },
        "1b4c9cddfd4f4f89a64c40c1e429bd5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7913cb1d27834e4cbae673e3baab5fe9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 15/15 [10:57&lt;00:00, 43.82s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21aa0e6d03cc4711a557ee82ddf372cd"
          }
        },
        "4310251017ae4a6fbd275a6d69ecf1c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40103217096f473c93e488cd15eaa2a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7913cb1d27834e4cbae673e3baab5fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21aa0e6d03cc4711a557ee82ddf372cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KrNfVNueNhR"
      },
      "source": [
        "# DialoGPT fine-tuning Chatbot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLaJbr59BlYA"
      },
      "source": [
        "## Set up drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgL1yLLS09mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1785ee-f1cb-4bb4-83fd-ddb66478bb03"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onU41i8g1J3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d29def4-220d-4cd4-cbde-0c666e585222"
      },
      "source": [
        "! pip -q install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4MB 15.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 51.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 53.7MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHNacD0k1HI3"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg8RH0-27beq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee904ee3-9a0f-4c94-89f2-d9ee17d836df"
      },
      "source": [
        "# cd Colab/\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cached\n",
            "data-eng\n",
            "data-eng.zip\n",
            "data.zip\n",
            "gpt2-results-conversation.txt\n",
            "gpt2-results.txt\n",
            "__MACOSX\n",
            "nltk_data\n",
            "output-small\n",
            "report2.gdoc\n",
            "RickAndMortyScripts.csv\n",
            "runs\n",
            "shapenetcore_partanno_segmentation_benchmark_v0.zip\n",
            "ssd-data.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hyTyW7Le2Sb"
      },
      "source": [
        "## First dialogue with DialoGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEqpXb4ofFGd"
      },
      "source": [
        "We will conduct all our experiments in Google Colab, its resources are enough to train the small DialoGPT model. Firstly, we will connect to Google Drive and install the necessary modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7Y1XcsqgXBQ"
      },
      "source": [
        "Let's move to the desired folder in which we will store all our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVSTlysV2jaM"
      },
      "source": [
        "Try to chat with DialoGPT without fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6qrl7_SvPKg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267,
          "referenced_widgets": [
            "6d0444a6a79b49ceb5e0cb080a32d15d",
            "88e6c5d1b3024abda6e125261bb588db",
            "8298e6a17ff046b1a3c94b1757b0de03",
            "61cf083be32745c285f19cfec6b5e4d7",
            "0edf85433c0c44c19764c68fddf8c772",
            "a959e533491b4c158a64e406851d593a",
            "66253844087d459babaf61c3f4f1e2ac",
            "ed79608317e74004ae009c8237e88998",
            "2eed819c3df34d34abd97f1ddb1f4a30",
            "2ef500b8fe5949939e762417d336d4dc",
            "539b0732ffd44f0a871ad88596612d6b",
            "2f2fad4c4e674062b09792c7b4aff5d8",
            "4ee2b429ea7c4690bdcfaece23ce0106",
            "bc2ec07ae6d54383a1f70ca428ab0fa6",
            "0b5bcdd7660c49e09c17e5d3d9128ddf",
            "bbc6db6f6a474cc883adcfa644686ed4",
            "94b0087ed7c34ee6839a2fac6941aae1",
            "32b147b9d12d4209acc6921449624e76",
            "b59a022c45ee483cb179fd79eb49d2a2",
            "00cbf796c9e64836b1ed6d981a95694e",
            "e6c73e0028c64cd79b4256ae6e27faa0",
            "7c81f46882f2483ba7ae51aa241d27e9",
            "4c1c1ed302474e0e82b97fa82f27dba0",
            "b2875ad278a84915bef0a3bc255565b5",
            "2d6fed40440140ffa38163cf29b0ac42",
            "84c10d0d08734f88a12604a9b6d8beeb",
            "948b24a8bbc4470dbae294f5da7d07ea",
            "2ac0c95b66cb48c48e59c93f626ab0bd",
            "2ab2ce6738434e2ab7cac1d617b1e7b5",
            "8e755f8252c544e3940692d8374523de",
            "76bf9b49716e4a04a4875cc75516f846",
            "ec596280489f4b5baf64aba50679d0ee"
          ]
        },
        "outputId": "fdd88548-0d3a-4770-8992-38fa6f38ef24"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d0444a6a79b49ceb5e0cb080a32d15d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=641.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2eed819c3df34d34abd97f1ddb1f4a30",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94b0087ed7c34ee6839a2fac6941aae1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/models/auto/modeling_auto.py:852: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d6fed40440140ffa38163cf29b0ac42",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=351265583.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjZaN5ilgd-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d334e2-0663-4199-ce5c-8cc0d2109bf5"
      },
      "source": [
        "# Let's chat for 5 lines\n",
        "for step in range(5):\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "    # generated a response while limiting the total chat history to 1000 tokens    \n",
        "    chat_history_ids = model.generate(\n",
        "    bot_input_ids, max_length=1000,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # pretty print last ouput tokens from bot\n",
        "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> User:hello\n",
            "DialoGPT: Hi\n",
            ">> User:feeling sick\n",
            "DialoGPT: What's up?\n",
            ">> User:what do i do\n",
            "DialoGPT: hi there\n",
            ">> User:do i need to get tested\n",
            "DialoGPT: Hi there\n",
            ">> User:what\n",
            "DialoGPT: Hi there\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuzSROqxjUKM"
      },
      "source": [
        "## Model initial configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC3qNlfp30aU"
      },
      "source": [
        "For start, we will need basic configuration and a dataset.\n",
        "Configuration and training scripts are mostly based on this [script](https://github.com/huggingface/transformers/tree/master/examples/language-modeling) from Huggingface and great [tutorial](https://nathancooper.io/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html) from Nathan Cooper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g91QzdqU2haO"
      },
      "source": [
        "\"\"\"\n",
        "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
        "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
        "using a masked language modeling (MLM) loss.\n",
        "\"\"\"\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from transformers import (\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except ImportError:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "# Configs\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utprDGf06OVt"
      },
      "source": [
        "# Args to allow for easy convertion of python script to notebook\n",
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.output_dir = 'output-small-save'\n",
        "        self.model_type = 'gpt2'\n",
        "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
        "        self.config_name = 'microsoft/DialoGPT-small'\n",
        "        self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
        "        self.cache_dir = 'cached'\n",
        "        self.block_size = 512\n",
        "        self.do_train = False\n",
        "        self.do_eval = True\n",
        "        self.evaluate_during_training = False\n",
        "        self.per_gpu_train_batch_size = 4\n",
        "        self.per_gpu_eval_batch_size = 4\n",
        "        self.gradient_accumulation_steps = 1\n",
        "        self.learning_rate = 5e-5\n",
        "        self.weight_decay = 0.0\n",
        "        self.adam_epsilon = 1e-8\n",
        "        self.max_grad_norm = 1.0\n",
        "        self.num_train_epochs = 3\n",
        "        self.max_steps = -1\n",
        "        self.warmup_steps = 0\n",
        "        self.logging_steps = 1000\n",
        "        self.save_steps = 3500\n",
        "        self.save_total_limit = None\n",
        "        self.eval_all_checkpoints = False\n",
        "        self.no_cuda = False\n",
        "        self.overwrite_output_dir = True\n",
        "        self.overwrite_cache = True\n",
        "        self.should_continue = False\n",
        "        self.seed = 42\n",
        "        self.local_rank = -1\n",
        "        self.fp16 = False\n",
        "        self.fp16_opt_level = 'O1'\n",
        "\n",
        "args = Args()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzUQGhffFknn"
      },
      "source": [
        "## Prepare Covid dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaAqugbGG9XD"
      },
      "source": [
        "import json\n",
        "\n",
        "f_train = open(\"data-eng/train_data.json\")\n",
        "train_data = json.load(f_train)\n",
        "f_train.close()\n",
        "# print(len(train_data))\n",
        "\n",
        "\n",
        "f_validate = open(\"data-eng/validate_data.json\")\n",
        "validate_data = json.load(f_validate)\n",
        "f_validate.close()\n",
        "# print(len(validate_data))\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk126BWvS_bP"
      },
      "source": [
        "train_contexted = []\n",
        "train_data = train_data\n",
        " \n",
        "for i in range(len(train_data)):\n",
        "  row = []\n",
        "  row.append(train_data[i][1])\n",
        "  row.append(train_data[i][0])\n",
        "  train_contexted.append(row)  "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2J587UqknoE"
      },
      "source": [
        "validate_contexted = []\n",
        "\n",
        "for i in range(len(validate_data)):\n",
        "  row = []\n",
        "  row.append(validate_data[i][1])\n",
        "  row.append(validate_data[i][0])\n",
        "  validate_contexted.append(row)  "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVh4Ch2dG0Q2",
        "outputId": "5b02d0a6-f439-4026-e964-2f50fd554344"
      },
      "source": [
        "columns = ['response', 'context'] \n",
        "columns = columns + ['context/'+str(i) for i in range(0)]\n",
        "columns"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['response', 'context']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAiUuAfnG1zw"
      },
      "source": [
        "# df = pd.DataFrame.from_records(contexted, columns=columns)\n",
        "# df.head(5)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "eBnN4DGrk2qp",
        "outputId": "e1961ac5-7fbf-412c-880e-5fdd3118a6ad"
      },
      "source": [
        "len(train_contexted)\n",
        "trn_df = pd.DataFrame.from_records(train_contexted, columns=columns)\n",
        "trn_df.head(5)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hello, I understand your concern. I just have ...</td>\n",
              "      <td>Hello doctor, I get a cough for the last few d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hello, I can understand your concern.In my opi...</td>\n",
              "      <td>Hello doctor, I am suffering from coughing, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hello. Anxiety can manifest itself in physical...</td>\n",
              "      <td>Hello doctor,I am a 23-year-old man. I have an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello,please answer the following:Any travel h...</td>\n",
              "      <td>Hello doctor,Last night I was getting chills, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hello and welcome to Ask A Doctor service.I ha...</td>\n",
              "      <td>Hi, I am Chaitanya, 27 years old. I use to swi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            response                                            context\n",
              "0  Hello, I understand your concern. I just have ...  Hello doctor, I get a cough for the last few d...\n",
              "1  Hello, I can understand your concern.In my opi...  Hello doctor, I am suffering from coughing, th...\n",
              "2  Hello. Anxiety can manifest itself in physical...  Hello doctor,I am a 23-year-old man. I have an...\n",
              "3  Hello,please answer the following:Any travel h...  Hello doctor,Last night I was getting chills, ...\n",
              "4  Hello and welcome to Ask A Doctor service.I ha...  Hi, I am Chaitanya, 27 years old. I use to swi..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ZAmxSK02G6_7",
        "outputId": "a36e5298-7e7f-4465-95e6-074a3712da76"
      },
      "source": [
        "len(validate_contexted)\n",
        "val_df = pd.DataFrame.from_records(validate_contexted, columns=columns)\n",
        "val_df.head(5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Corona-virus. At 33 you may not need testing. ...</td>\n",
              "      <td>I have a constant cough and my chest has now b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Less likely. Recommended to stay 6 feet apart....</td>\n",
              "      <td>If someone has carona virus and iam passing by...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Test   Please stay at home, rest, drink fluids...</td>\n",
              "      <td>I am concerned that I’m showing symptoms of co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Death. At your age the risk of death is the fo...</td>\n",
              "      <td>What are my chances of becoming seriously ill ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Unknown but low   Based on current data it is ...</td>\n",
              "      <td>Nervous about coronavirus. I am 26 years old a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            response                                            context\n",
              "0  Corona-virus. At 33 you may not need testing. ...  I have a constant cough and my chest has now b...\n",
              "1  Less likely. Recommended to stay 6 feet apart....  If someone has carona virus and iam passing by...\n",
              "2  Test   Please stay at home, rest, drink fluids...  I am concerned that I’m showing symptoms of co...\n",
              "3  Death. At your age the risk of death is the fo...  What are my chances of becoming seriously ill ...\n",
              "4  Unknown but low   Based on current data it is ...  Nervous about coronavirus. I am 26 years old a..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1155nQJ_GShb"
      },
      "source": [
        "## Dataset setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86F3WhnFO4H8"
      },
      "source": [
        "Now will convert our dataset in a format suitable for our model. Basically we will concatenate responses in one string for each row (additionally we will add special 'end of string' token between responses, so the model will understand end of each response in a string).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX7jeWpYmOe_"
      },
      "source": [
        "def construct_conv(row, tokenizer, eos = True):\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
        "    conv = flatten(conv)\n",
        "    return conv\n",
        "\n",
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
        "\n",
        "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
        "\n",
        "        directory = args.cache_dir\n",
        "        cached_features_file = os.path.join(\n",
        "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
        "        )\n",
        "\n",
        "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, \"rb\") as handle:\n",
        "                self.examples = pickle.load(handle)\n",
        "        else:\n",
        "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
        "\n",
        "            self.examples = []\n",
        "            for _, row in df.iterrows():\n",
        "                conv = construct_conv(row, tokenizer)\n",
        "                self.examples.append(conv)\n",
        "\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, \"wb\") as handle:\n",
        "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item], dtype=torch.long)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naaRHoXgnStq"
      },
      "source": [
        "# Cacheing and storing of data/checkpoints\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
        "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
        "    ordering_and_checkpoint_path = []\n",
        "\n",
        "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
        "\n",
        "    for path in glob_checkpoints:\n",
        "        if use_mtime:\n",
        "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
        "        else:\n",
        "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
        "            if regex_match and regex_match.groups():\n",
        "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
        "\n",
        "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
        "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
        "    return checkpoints_sorted\n",
        "\n",
        "\n",
        "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
        "    if not args.save_total_limit:\n",
        "        return\n",
        "    if args.save_total_limit <= 0:\n",
        "        return\n",
        "\n",
        "    # Check if we should delete older checkpoint(s)\n",
        "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
        "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
        "        return\n",
        "\n",
        "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
        "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
        "    for checkpoint in checkpoints_to_be_deleted:\n",
        "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
        "        shutil.rmtree(checkpoint)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "_Iv3O_nmDpqw",
        "outputId": "68566cc0-325b-451b-c931-05592378f2f8"
      },
      "source": [
        "trn_df"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hello, I understand your concern. I just have ...</td>\n",
              "      <td>Hello doctor, I get a cough for the last few d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hello, I can understand your concern.In my opi...</td>\n",
              "      <td>Hello doctor, I am suffering from coughing, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hello. Anxiety can manifest itself in physical...</td>\n",
              "      <td>Hello doctor,I am a 23-year-old man. I have an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello,please answer the following:Any travel h...</td>\n",
              "      <td>Hello doctor,Last night I was getting chills, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hello and welcome to Ask A Doctor service.I ha...</td>\n",
              "      <td>Hi, I am Chaitanya, 27 years old. I use to swi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>Quarantine   You should be in self quarantine ...</td>\n",
              "      <td>Girlfriend has coronavirus. With bad symptoms....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>Fever with body ache   Hello &amp; welcome to Heal...</td>\n",
              "      <td>Scratching sore thoat and  slight chest irrita...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>Cough,phlegm. At this time your symptoms are c...</td>\n",
              "      <td>Associated with phlegm and mucus?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>Correct. Every bit helps.</td>\n",
              "      <td>Should one also cover eyes in addition to cove...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>Self quarantine    Suggest self quarantine for...</td>\n",
              "      <td>Should I be tested for COVID 19 have been in c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>483 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              response                                            context\n",
              "0    Hello, I understand your concern. I just have ...  Hello doctor, I get a cough for the last few d...\n",
              "1    Hello, I can understand your concern.In my opi...  Hello doctor, I am suffering from coughing, th...\n",
              "2    Hello. Anxiety can manifest itself in physical...  Hello doctor,I am a 23-year-old man. I have an...\n",
              "3    Hello,please answer the following:Any travel h...  Hello doctor,Last night I was getting chills, ...\n",
              "4    Hello and welcome to Ask A Doctor service.I ha...  Hi, I am Chaitanya, 27 years old. I use to swi...\n",
              "..                                                 ...                                                ...\n",
              "478  Quarantine   You should be in self quarantine ...  Girlfriend has coronavirus. With bad symptoms....\n",
              "479  Fever with body ache   Hello & welcome to Heal...  Scratching sore thoat and  slight chest irrita...\n",
              "480  Cough,phlegm. At this time your symptoms are c...                  Associated with phlegm and mucus?\n",
              "481                          Correct. Every bit helps.  Should one also cover eyes in addition to cove...\n",
              "482  Self quarantine    Suggest self quarantine for...  Should I be tested for COVID 19 have been in c...\n",
              "\n",
              "[483 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkvMNnrnVHQw"
      },
      "source": [
        "## Training and Evaluating\n",
        "\n",
        "There will be quite a lot of code needed for training our model but don’t worry, everything should work as is, the main thing is to give the model the dataset in the right format.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXzKlXHeu0Mb"
      },
      "source": [
        "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
        "    )\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    # add_special_tokens_(model, tokenizer)\n",
        "\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if (\n",
        "        args.model_name_or_path\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
        "        )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps\n",
        "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
        "        try:\n",
        "            # set global_step to gobal_step of last saved checkpoint from model path\n",
        "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "            global_step = int(checkpoint_suffix)\n",
        "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "        except ValueError:\n",
        "            logger.info(\"  Starting fine-tuning.\")\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
        "    )\n",
        "    set_seed(args)  # Added here for reproducibility\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            inputs, labels = (batch, batch)\n",
        "            if inputs.shape[1] > 1024: continue\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    if (\n",
        "                        args.local_rank == -1 and args.evaluate_during_training\n",
        "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    checkpoint_prefix = \"checkpoint\"\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
        "                    os.makedirs(output_dir, exist_ok=True)\n",
        "                    model_to_save = (\n",
        "                        model.module if hasattr(model, \"module\") else model\n",
        "                    )  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
        "\n",
        "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "# Evaluation of some model\n",
        "\n",
        "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
        "    os.makedirs(eval_output_dir, exist_ok=True)\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
        "    )\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        inputs, labels = (batch, batch)\n",
        "        inputs = inputs.to(args.device)\n",
        "        labels = labels.to(args.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            lm_loss = outputs[0]\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
        "\n",
        "    result = {\"perplexity\": perplexity}\n",
        "\n",
        "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return result"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MGD6bFXV4Z-"
      },
      "source": [
        "# Main runner\n",
        "\n",
        "def main(df_trn, df_val):\n",
        "    args = Args()\n",
        "    \n",
        "    if args.should_continue:\n",
        "        sorted_checkpoints = _sorted_checkpoints(args)\n",
        "        if len(sorted_checkpoints) == 0:\n",
        "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
        "        else:\n",
        "            args.model_name_or_path = sorted_checkpoints[-1]\n",
        "\n",
        "    if (\n",
        "        os.path.exists(args.output_dir)\n",
        "        and os.listdir(args.output_dir)\n",
        "        and args.do_train\n",
        "        and not args.overwrite_output_dir\n",
        "        and not args.should_continue\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "                args.output_dir\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    device = torch.device(\"cuda\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        args.local_rank,\n",
        "        device,\n",
        "        args.n_gpu,\n",
        "        bool(args.local_rank != -1),\n",
        "        args.fp16,\n",
        "    )\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args)\n",
        "\n",
        "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
        "    model = AutoModelWithLMHead.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        from_tf=False,\n",
        "        config=config,\n",
        "        cache_dir=args.cache_dir,\n",
        "    )\n",
        "    model.to(args.device)\n",
        "    \n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
        "\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "    if args.do_train:\n",
        "        # Create output directory if needed\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = (\n",
        "            model.module if hasattr(model, \"module\") else model\n",
        "        )  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\n",
        "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
        "        model.to(args.device)\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "        checkpoints = [args.output_dir]\n",
        "        if args.eval_all_checkpoints:\n",
        "            checkpoints = list(\n",
        "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
        "            )\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
        "\n",
        "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
        "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "    return results"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K-Smri694kl"
      },
      "source": [
        "## Start Fine-Tuning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZEHDzR0Vjs7"
      },
      "source": [
        "It is time to train our model!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__iqR8YFV-Ex",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "e76773810d714ad9b4d9954f37c53bff",
            "97e3a1002cc748babfe474355a17d1be",
            "064459aa1bde4b9388bfde309dde6a93",
            "1b4c9cddfd4f4f89a64c40c1e429bd5f",
            "4310251017ae4a6fbd275a6d69ecf1c2",
            "40103217096f473c93e488cd15eaa2a8",
            "7913cb1d27834e4cbae673e3baab5fe9",
            "21aa0e6d03cc4711a557ee82ddf372cd"
          ]
        },
        "outputId": "41faffa7-17df-468d-a591-42bb4b5157ab"
      },
      "source": [
        "main(trn_df, val_df)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/04/2020 04:50:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/models/auto/modeling_auto.py:852: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "12/04/2020 04:50:53 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7f44e0c81978>\n",
            "12/04/2020 04:50:53 - INFO - __main__ -   Evaluate the following checkpoints: ['output-small-save']\n",
            "12/04/2020 04:50:59 - INFO - __main__ -   Creating features from dataset file at cached\n",
            "12/04/2020 04:50:59 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
            "12/04/2020 04:50:59 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "12/04/2020 04:50:59 - INFO - __main__ -     Num examples = 60\n",
            "12/04/2020 04:50:59 - INFO - __main__ -     Batch size = 4\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e76773810d714ad9b4d9954f37c53bff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=15.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "12/04/2020 04:51:00 - INFO - __main__ -   ***** Eval results  *****\n",
            "12/04/2020 04:51:00 - INFO - __main__ -     perplexity = tensor(14.2759)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'perplexity_': tensor(14.2759)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40rKH7FxJaHM"
      },
      "source": [
        "## Generate test results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_wcWTNKMfW3",
        "outputId": "5b33075d-b906-4cca-e686-59b5290126a8"
      },
      "source": [
        "f_test = open(\"data-eng/test_data.json\")\n",
        "test_data = json.load(f_test)\n",
        "f_test.close()\n",
        "\n",
        "test_query = []\n",
        "test_response = []\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "  test_response.append(test_data[i][1])\n",
        "  test_query.append(test_data[i][0])\n",
        "\n",
        "print(len(test_response))\n",
        "print(len(test_query))\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61\n",
            "61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1DwhiKDJZVZ",
        "outputId": "fff6d39d-63a4-423c-d479-95e963814a9b"
      },
      "source": [
        "test_chatbot = []\n",
        "\n",
        "for i in range(len(test_query)):\n",
        "  tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
        "  model = AutoModelWithLMHead.from_pretrained('output-small')\n",
        "  # append the new user input tokens to the chat history\n",
        "  bot_input_ids = tokenizer.encode(test_query[i] + tokenizer.eos_token, return_tensors='pt')\n",
        "  print(\"User: {} \\n\".format(test_query[i]))\n",
        "\n",
        "  # generated a response while limiting the total chat history to 1000 tokens, \n",
        "  chat_history_ids = model.generate(\n",
        "      bot_input_ids, max_length=100,\n",
        "      pad_token_id=tokenizer.eos_token_id,  \n",
        "      no_repeat_ngram_size=3,       \n",
        "      do_sample=True, \n",
        "      top_k=10, \n",
        "      top_p=0.7,\n",
        "      temperature = 0.8\n",
        "  )\n",
        "\n",
        "  # pretty print last ouput tokens from bot\n",
        "  print(\"Chatbot: {} \\n\\n\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "  test_chatbot.append(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True))\n",
        "\n",
        "print(len(test_chatbot))\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/models/auto/modeling_auto.py:852: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "I have all the symptoms except fever, I went to Medicross and Dr said I can get tested if I want to I'm not sure if I should. She gave me antibiotics Klacid XL 500mg, she said I can take it if I feel worse I'm worried it will make immune system bad?\n",
            "Chatbot: I have a feeling you are not getting tested.    You can get test if you have a fever or if you are feeling sick, but if you don't you are probably\n",
            "I have pain/discomfort in my lungs. I don't experience simultaneous on both lungs and it not always at the hame position. I don't have a head nor do I have high temperature. I sneeze and cough maybe once a day. Do I have corona, should I get tested?\n",
            "Chatbot: Yes. Yes. Yes, you are fine.\n",
            "I travelled to Mauritius and do not have symptoms. Should I get tested for covid19?\n",
            "Chatbot: Yes, if you are symptomatic. You should get tested.    Would you like to video or text chat with me?\n",
            "I have a dry cough and a sore throat. I do not have a fever and a slight headache (could be I need more water) I am 4 months pregnant and not sure If I should be tested for Corona?\n",
            "Chatbot: No. You should be fine.\n",
            "I went to the Dr for a cough and fever on march 5. They diagnosed me with bronchitis. I was not doing any better, so I returned to the Dr on march 15. They sent me to the ER to get tested for covid. At the er they ran a number of tests including a covid t.\n",
            "Chatbot: Thanks for your question on Healthcare Magic.I have a question about the bronchoscopy. What do you think is better to get done? I have been\n",
            "What’s worse with the coronavirus, type 1 or type 2 diabetes?\n",
            "Chatbot: In my opinion, the coronovirus is worse.    I am a diabetic and have had it for 4 months.  The coronaviruses are more common.  Would you like to video or text chat with me?\n",
            "Just found out that my MD tested positive for Covid-19. I had direct interaction with him. He has no symptoms, he was tested because he traveled in February to Switzerland. Should I be tested. I’m 67, or wait to see if I develop any symptoms.?\n",
            "Chatbot: Hello and Welcome to Healthcare Magic.Ask your doctor if you can be tested and if so do so.  Would you like to video or text chat with me?\n",
            "Hi. My children and husband were at the doctor yesterday and he has now informed us he has tested positive for covid19 . He has come home to my 2 babies and mom. How long before we can go for a test?\n",
            "Chatbot: Hello and Welcome to Healthcare Magic.com. I can understand your concern. I would suggest taking a test and seeing a doctor if you have a fever, and if you are concerned, get tested for covids. If you have fever, or have been exposed\n",
            "We went to bali via singapore, on my second day in bali (17 March) I started feeling a bit sick?\n",
            "Chatbot: Hello,    You should stay home,  If you are not in contact with others or stay at home, you are more likely to be infected.   If the symptoms persist, contact your doctor for evaluation.  Would you like to video or text chat with me?\n",
            "Is covering mouth and nose by mask sufficient to avoid corona exposure? Can It not be transmitted through eyes and ears?\n",
            "Chatbot: It is not a virus. It is a virus that has been transmitted through the nose, mouth, and mouth.\n",
            "I have scratchy throat, phlegm, loss of appetite, fatigue. I am pharmacist in research facility. My son had 3rd contact. He has flu symptoms. He was quarantined. We are waiting for the results for his contact. Do I need to test myself?\n",
            "Chatbot: Yes. You are not a doctor. You can test for the flu, but you are not required to do so. You should definitely consult a doctor to get tested.\n",
            "Can animal byproducts spread coronavirus? Do I have to worry about milk, eggs, or fruits?\n",
            "Chatbot: Covid virus.    The coronaviruses are not known to cause the coronavira virus.\n",
            "Why are so many people with diabetes dying from the coronavirus? It’s really frightening me.\n",
            "Chatbot: Yes. The coronaviruses are not known to cause pneumonia. In the past, coronavid-19, the coronas have been used as a treatment for people with pneumonia.\n",
            "I'm just a bit concerned as I have a sore throat and coughing a little bit as a result of my sore throat. My upper body is also aching a bit. Is this a cause for concern?\n",
            "Chatbot: Not a problem.    Would you like to video or text chat with me?\n",
            "Will Lysol wipes kill coronavirus? How long can coronavirus live on household surfaces?\n",
            "Chatbot: Yes. If you have a fever and a cough you should not use any products that are aerosolized. You should also use a spray like Sulfur spray or aerosol spray.\n",
            "Can I catch coronavirus on public transportation from someone who has it? How long do germs on public transportation last?\n",
            "Chatbot: No. You should not catch coronovirus. You can get coronaviruses at home, but you should not be catching coronavitivirus. The coronavitis virus is transmitted through airways, which are very important for people who have access to public transportation.\n",
            "Is it necessary to get tested for the corona virus if you arrive back in South Africa after visiting Zimbabwe and Victoria falls as part of a group?\n",
            "Chatbot: In the US, it depends on where you are and if you have traveled to a country that has been in contact with someone who has travelled to South Africa or if you are in contact via a phone call with someone that has traveled to South African country.\n",
            "Hi. This COVID-19 outbreak is scary. I got screened this week and it was negative. But prior to screening I had a week of continuous dry coughs and also throat was sore. I've put myself in a quarantine. What next? Do I still need to screen again?\n",
            "Chatbot: If you are having a dry cough, you have had COVID 19, which is not a virus. If you have a dry throat, you are not having a cough, and you are at a\n",
            "Is it true (or a myth) that coronavirus will not survive hotter climates. As summer approaches, is it expected to reduce in tropical summers at least.\n",
            "Chatbot: It is unlikely to survive in hot places. As in hot areas.\n",
            "I heard ACE inhibitors might exacerbate coronavirus. I also heard NSAIDs might do the same. Does this mean any common meds could have an unexpected effect during coronavirus?\n",
            "Chatbot: No.    It is not a controlled study and you will not have any data to support this claim.  Would you like to video or text chat with me?\n",
            "I have had mild chest pain for over a week. it now seems more persistent and pronounced. I don't have shortness of breath or any other COVID-19 symptoms, except some fatigue. I have been traveling a lot in high risk areas. Should I get tested?\n",
            "Chatbot: No. You should definitely get tested.\n",
            "Ons het teruggekom vanaf oostenryk op 8 Maart..reeds 13 dae gelede. Het tans seer keel maar ook nasale drip. wat staan my te doen?\n",
            "Chatbot: Hello and Welcome to the ‘    ‘ Healthcare Magic Magic Magic. I have reviewed your query and will be happy to assist you further. Let me know if I can assist you.\n",
            "I have shortness of breath and are worried, I don’t have a cough or sore throat, so they will not test me, should I do a private test?\n",
            "Chatbot: Hello and Welcome to Healthcare Magic.I can understand your concern.I have a fever and a slight cough but no fever. I can understand you concern.Hello and welcome to Healthcaremagic.com I can assist you in your query.Regards,Dr.Dr.Dalekkhar\n",
            "How do you get HPV on the cervix? Is it from skin to skin contact during intercourse from a partner’s genital touching the cervix? Thank you.\n",
            "Chatbot: Covid 19, not sure if you have a partner or not, but if you do, get tested for it.\n",
            "My 2year old Son has had a dry cough and a runny nose since thursday . He's not talking yet so it is difficult to tell if he has any trouble breathing so I'm concerned. He does not have temperature ?\n",
            "Chatbot: I can understand your concern. I have had the same issue with a cough. My son has a runnny nose and is currently experiencing a dry throat. I can understand the concern.\n",
            "Can I test?No known contact with Corona  but been in contact with with Congo dr friend 7march. She is still healthy now. I had Slight Sore throat wed 11 &12 March . phlegm and red eyes. Temp seemed ok. Got thermometer .ok. 19 felt better but coughing.\n",
            "Chatbot: Hello and Welcome to RAOA. I have reviewed your query and here is my advice for you: test your immune system.\n",
            "I’m so Scared that I will die if I get the coronavirus, due to type1 diabetes?\n",
            "Chatbot: No. It is not a virus. It has no effect. It only causes the coronacosis.\n",
            "A 2 year complaining that his head is sore. We gave him panado in the morning and at midday. He can't sleep he keeps waking up crying its painful. Should I take him to a Dr or hospital?\n",
            "Chatbot: Yes, it is a virus. He is in a hospital and the doctor is concerned that it is spreading. You should contact a local health department and see if they can assist you.\n",
            "I have covid 19 symptoms?\n",
            "Chatbot: Not sure if you have covids.  Would you like to video or text chat with me?\n",
            "I've been in contact with a possible covid-19 case who is waiting on results. My brother seems to be I'll with Corona symptoms and I'd like to get advice on how to help him, as his safety will ensure my safety and prevent the spread of the virus.\n",
            "Chatbot: Hello and Welcome to Healthcare Magic.Welcome to Healthcaremagic.com.Hope I have answered your query. I have sent you a query.\n",
            "I have mild irritation in my chest but I am not coughing, I just feel a tingling sensation on my thought. I have no fever, no aches should I be worried about COVID 19?\n",
            "Chatbot: It depends on your symptoms, but if you have a fever, you should be alright.\n",
            "Swollen tonsils, cough, and feeling slightly feverish.\n",
            "Chatbot: Hello and Welcome to RAOA, What has been causing your symptoms, and what is the reason you are having this problem?\n",
            "I have a sore throat  and a bit of a cough. This morning i coughed up mucus with what look like blood. Is this a sign of covid 19?\n",
            "Chatbot: No.    The cough is very common and the cough is not very uncommon.  If you cough, it is likely that you have a virus.  You should get tested for Covid 19. \n",
            "Symptoms include irritation in the throat, dry cough, trouble breathing properly, headache. However not really a sign of a fever. Temperature is about 35 degrees. Should I get tested?\n",
            "Chatbot: No fever or other symptoms, but the symptoms are not a sign. You are not in a good place to get tested for COVID-19. If you have symptoms of COVID, you should get tested.\n",
            "I am 63 living in Jhb and have had a tight chest and sore throat and wet cough for more than a week ?\n",
            "Chatbot: Yes. You are more likely to have a cough and dry cough.    You should be alright.  You are not in a home.  If you are in a high risk area, stay home.\n",
            "Chronic sore throat for 4 months. Not strep. No fever. Bloody green phlegm for a while. Sinus congestion and pressure sometimes. Bad breath in back of throat sometimes. Maybe post nasal drip. 2 wks. Of amoxicillin didn’t help. Is this mild COVID-19?\n",
            "Chatbot: Hello and Welcome to the Healthcare Magic Magic Magic. Please read the FAQ and FAQ on Healthcare Magic. I have reviewed your query and here is my advice for you:\n",
            "Been sneezing / stuffy nose and slight sore throat since returning from France, temperature is okay, not coughing or high fever symptoms, should I be alarmed?\n",
            "Chatbot: Hello and Welcome to RAOA. You may want to video or text chat with us. Stay safe and stay home.\n",
            "Can I go for Coronavirus testing if my nose is blocked and I have traveled from a high risk country?\n",
            "Chatbot: Yes, you should definitely do this. The risk of corona is high. You should definitely take it to a local health department.\n",
            "My 5 year old daughter has just come down with a 39 degree fever, sneezing, runny nose and cough. Should we get her tested?\n",
            "Chatbot: Yes. It is not an infection. The symptoms are similar to pneumonia.    It is also possible to be a person with pneumonia. However, you will not be able to get tested for pneumonia. If you do get tested you will be tested for COVID-19, but you will need to go to a doctor. \n",
            "I am a quadriplegic and have a tight chest and slight difficulty breathing.  Overnight I started feeling like I need to clear my throat. I flew home to Joburg from CPT 8 days ago & have been self isolating as a precaution. ?\n",
            "Chatbot: Possible.    Would you like to video or text chat with me?\n",
            "Hi I am 39 years old and returned from Germany 19 days ago. Yesterday I started getting a sore throat, runny nose. Today I have sinus pressure and a headache with a blocked nose, throat seems to be improving. Should I get tested. If so how?\n",
            "Chatbot: Yes, it will improve your symptoms.    If you get a fever or runny throat or cough then you will get tested for COVID-19.  If it does not improve your cough then the symptoms\n",
            "Hi I am 39 years old and have flu like symptoms as of yesterday. Runny nose, sore throat. No fever yet. I returned from Germany 19 days ago. Should I get tested for cov19?\n",
            "Chatbot: Hello and Welcome to Healthcare Magic.   You can use the Magic Magic Magic. It is a free service that is available at your local Healthcare Magic Magic center. You can check out the Magicmagicmagic.com for free Healthcare Magic and get tested.  If you have a fever\n",
            "If you have wolf Parkinson white syndrome does this make you more of a risk with Covid 19?\n",
            "Chatbot: No, you can get it from the vaccine. The vaccine is a virus that causes your immune system to be able to protect you. The virus is not contagious and you are not exposed to it. The only way to protect yourself is to get tested.\n",
            "Criteria for COVID test?\n",
            "Chatbot: Not a COVID Test. It is not a known issue that you should be tested for it.\n",
            "I have been feeling sick for a couple of days now and tried self medication but does not seem to help. I suffer from Polycystic Kidney disease, low blood pressure, Anemic, Endometriosis, Chronic sinus.\n",
            "Chatbot: Hello and Welcome to r askhealthcare.Regards,Dr.PFT.\n",
            "I have had a runny nose and have been coughing and have had on and off headaches. I went to the doctor the first time, he gave me an antibiotic and i got worse then I went back a and he gave me a different one but it seems I still have these symptoms?\n",
            "Chatbot: Coughing, headache, or dry cough.\n",
            "I am experiencing headache (severity: moderate) (time frame: gradual) (occurred: 1 - 3 hours) (side: front) (worsened by: psychological stress, being tired, menstruation, skipping meals) (unaffected by: bending forward, changes in sleep pattern, sneeze or.\n",
            "Chatbot: Hello and Welcome to RAOA! Welcome to r RAOA HealthPraying. I have reviewed your query and here is my advice. Please stay home. I\n",
            "This may sound silly but I’m wondering, if touch something in store that has Coronavirus 19 on it and it gets on clothes should we go home wash them soon as possible then shower and new clothes on so it dont get on our stuff at home ?\n",
            "Chatbot: No. The virus is spread by washing your clothes and washing your hands with soap and water after washing your feet and hands. You will not have any chance of catching it.\n",
            "Re Covid 19 I understand the incubation period of Covid 19 is up to 14 days. But how long is the contagious period? I have read one is contagious 2 days before showing symptoms. Is that true? Are you still contagious until symptom free?\n",
            "Chatbot: The virus has not spread. The virus is still spread.    Would you like to video or text chat with me?\n",
            "Hi I am a health worker and a chronic asthmatic, is it easier for me to contract Corona? Will I survive it with my comorbidity? Should I ask for special leave during this time? I already feel my sinus blocked, very minor sore throat, seldom dry cough.\n",
            "Chatbot: Corona is very common.    It is not a virus.  Would you like to video or text chat with me?\n",
            "Possible COVID-19 - mild symptoms - Next steps? Symptoms: - Initially: tight chest, stuffy nose - 10% difficulty breathing - Mildly feverish (hot and dry mouth, bit weak) for a few hours yesterday - Mostly fine this morning (5% breathing difficulty).\n",
            "Chatbot: Hello and Welcome to the Magic Magic Magic. Magic Magic is the best Magic.\n",
            "Throat a bit sore and want to get a good imune booster, especially in light of the virus. Please advise. Have not been in contact with Nyone with the virus.\n",
            "Chatbot: Hello, Contact with Nyon. If you have been in touch with a person with the disease and the virus, contact with them. If the virus has been spread by the virus itself, contact your doctor.\n",
            "I have had a sore throat (glands enlarged in throat) for over a week and I have constant headache. Do I need COVID-19 test. If yes, how do a get someone to come to my house to test.\n",
            "Chatbot: Hello and Welcome to r Covid-19. Hope I have answered your query. Please contact me via chat. I can assist you further.\n",
            "Do I just go to a Dr to determine if it's a sinus infection or do I go get tested for COVID first? To not risk anyone in practice?\n",
            "Chatbot: In short, no. COVID is a virus.    You should consult a physician for your treatment.  If you have access to a doctor, you should consult the doctor to ensure you are not in contact with someone with a viral infection.  Would you like to video or text chat with me?\n",
            "I have a dry cough and sore throat- it's been a week now and the cough seems to be getting worse- no runny nose or fever, sometimes a headache, no shortness of breath...should I get tested for covid19?\n",
            "Chatbot: Covid 19 is a very serious disease.    You can test for Covid 19.  Would you like to video or text chat with me?\n",
            "If you've been infected by SARS-CoV-2, develop Covid-19 symptoms and recover, can you: 1) develop immunity to Covid-19 2) infect others after 14 days post-symptom onset & recovery 3) get re-infected later by SARS-CoV-2 despite developing immunity?\n",
            "Chatbot: No.    If you develop immunity, develop immunity.  If not, consult your local medical facility, or go to the ER immediately\n",
            "If I get coronavirus, are there ways to avoid getting other illnesses as well, like pneumonia?\n",
            "Chatbot: Yes, you can get other illnesses.\n",
            "I was on a plane on March 8th have been home ever since self quarantine myself I have had headaches and runny throat I have 2 days left of my self quarantine should I be having worse symptoms by now ?\n",
            "Chatbot: No. You should definitely stay home. If you have a fever, you should stay home, but stay home and stay safe.\n",
            "My son has a fever, I am concerned due to the coronavirus ?\n",
            "Chatbot: No fever, no fever. You should consult your physician if you have a fever. If fever is not present, consult your doctor if fever is present, and if fever does not present you should consult a physician.\n",
            "Can coronavirus symptoms be mild for some people versus severe? For example, could it just involve being very fatigued, low grade fever for a few days and not the extreme symptoms? Or is it always a full blown cold and struggle to breathe?\n",
            "Chatbot: No. It depends on the type of virus. It is not a virus. You should stay home.\n",
            "Good day,  I am 29 weeks pregnant and my profession is teaching, I also have a 2 year old son.  My chest and throat is sore, I have a constant headache and feel dizzy at times. Is it necessary to get myself tested for the coronavirus?\n",
            "Chatbot: Yes.    You can get tested for coronaviruses.  You should also get tested.  Your doctor should also be able to tell you if you have a fever or if you are having\n",
            "61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRJJ3YcERdLF"
      },
      "source": [
        "with open('gpt2-results.txt', 'w') as f:\n",
        "    for i in test_chatbot:\n",
        "        f.write('Chatbot: %s\\n\\n' % i)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BmAO50tibOc"
      },
      "source": [
        "with open('gpt2-results-conversation.txt', 'w') as f:\n",
        "    for i in range(len(test_chatbot)):\n",
        "        f.write('User: %s\\n' % test_query[i])\n",
        "        f.write('Chatbot: %s\\n\\n' % test_chatbot[i])"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnV2P6PpWNUi"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "0wfYNahDfPBZ",
        "outputId": "b38cd912-02ea-4239-87cb-a0be5ae4dbc3"
      },
      "source": [
        "pip install \"nltk==3.4.5\""
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 21.7MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 27.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 27.8MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 20.3MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 19.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 14.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 13.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 14.0MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 13.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 13.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 13.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 13.0MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 13.0MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 624kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 645kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 655kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 665kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 931kB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 942kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 962kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 972kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 983kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 993kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4MB 13.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.15.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449904 sha256=330a508cd0d19d46796ead124c3d22109692f1dabbba95262cde9ecf94240589\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_6uMEd6WcLe"
      },
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.nist_score import sentence_nist\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_OPMDr4WO6t"
      },
      "source": [
        "def get_metrics(pred, target):\n",
        "    turns = len(target)\n",
        "    bleu_2 = 0\n",
        "    bleu_4 = 0\n",
        "    meteor = 0\n",
        "    nist_2 = 0\n",
        "    nist_4 = 0\n",
        "    for index in range(turns):\n",
        "        pred_utt = pred[index]\n",
        "        target_utt = target[index]\n",
        "        min_len = min(len(pred_utt), len(target_utt))\n",
        "        lens = min(min_len, 4)\n",
        "        if lens == 0:\n",
        "            continue\n",
        "        if lens >= 4:\n",
        "            bleu_4_utt = sentence_bleu([target_utt], pred_utt, weights = (0.25, 0.25, 0.25, 0.25), smoothing_function = SmoothingFunction().method1)\n",
        "            nist_4_utt = sentence_nist([target_utt], pred_utt, 4)\n",
        "        else:\n",
        "            bleu_4_utt = 0\n",
        "            nist_4_utt = 0\n",
        "        if lens >= 2:\n",
        "            bleu_2_utt = sentence_bleu([target_utt], pred_utt, weights = (0.5, 0.5), smoothing_function = SmoothingFunction().method1)\n",
        "            nist_2_utt = sentence_nist([target_utt], pred_utt, 2)\n",
        "        else:\n",
        "            bleu_2_utt = 0\n",
        "            nist_2_utt = 0\n",
        "            \n",
        "        bleu_2 += bleu_2_utt\n",
        "        bleu_4 += bleu_4_utt\n",
        "        meteor += meteor_score([\" \".join(target_utt)], \" \".join(pred_utt))\n",
        "        nist_2 += nist_2_utt\n",
        "        nist_4 += nist_4_utt\n",
        "        \n",
        "    bleu_2 /= turns\n",
        "    bleu_4 /= turns\n",
        "    meteor /= turns\n",
        "    nist_2 /= turns\n",
        "    nist_4 /= turns\n",
        "    return bleu_2, bleu_4, meteor, nist_2, nist_4\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-FjW-r-fczL"
      },
      "source": [
        "mkdir nltk_data"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZfpaUPaffzr",
        "outputId": "fe4b3c46-be84-4cb0-d6ea-f4f49ad7a0de"
      },
      "source": [
        "nltk.download('wordnet')\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuv4nCbpWhmo"
      },
      "source": [
        "  # bleu_2, bleu_4, meteor, nist_2, nist_4 = get_metrics(pred_token, target_token)\n",
        "  bleu_2, bleu_4, meteor, nist_2, nist_4 = get_metrics(test_chatbot, test_response)\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p59t0xOPfwLB",
        "outputId": "c9f2ffb4-b35d-467e-bb5f-e5830e2d61bf"
      },
      "source": [
        " bleu_2, bleu_4, meteor, nist_2, nist_4"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.19885228807178082,\n",
              " 0.10358792254313927,\n",
              " 0.22840520757502006,\n",
              " 0.9904637041692446,\n",
              " 1.0259282670432335)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eDkPEuvbD47"
      },
      "source": [
        "## Interactive Chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjVqotI05gOS"
      },
      "source": [
        "A variety of methods can be used in responces generation. You can find more details about these methods by this [link](https://huggingface.co/blog/how-to-generate). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIeqMwZktv7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c535af55-0a20-4d38-ab13-8cc7cbe0edd4"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
        "model = AutoModelWithLMHead.from_pretrained('output-small')\n",
        "\n",
        "# Let's chat for 5 lines\n",
        "for step in range(1):\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
        "    # print(new_user_input_ids)\n",
        "\n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "    # generated a response while limiting the total chat history to 1000 tokens, \n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids, max_length=100,\n",
        "        pad_token_id=tokenizer.eos_token_id,  \n",
        "        no_repeat_ngram_size=3,       \n",
        "        do_sample=True, \n",
        "        top_k=10, \n",
        "        top_p=0.7,\n",
        "        temperature = 0.8\n",
        "    )\n",
        "    \n",
        "    # pretty print last ouput tokens from bot\n",
        "    print(\"Chatbot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/models/auto/modeling_auto.py:852: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">> User:Hello doctor, I am suffering from coughing, throat infection from last week. At that time fever did not persist and also did not felt any chest pain. Two days later, I consulted with a doctor. He prescribed Cavidur 625, Montek LC, Ambrolite syrup and Betaline gargle solution. Since then throat infection improved and frequent cough also coming out. Coughing also improved remarkably though not completely. From yesterday onwards fever is occuring (maximum 100-degree Celcius). I have not come in touch with any foreign returned person nor went outside. In our state, there is no incidence of Covid-19. Please suggest what to do?\n",
            "Chatbot: \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}